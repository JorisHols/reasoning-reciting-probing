{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the experiment for different intervention strenghts (alpha) from -0.1 to 0.1 and evaluate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data for alpha=-0.10, found 800 examples\n",
      "Loaded data for alpha=-0.05, found 800 examples\n",
      "Loaded data for alpha=0.00, found 800 examples\n",
      "Loaded data for alpha=0.05, found 800 examples\n",
      "Loaded data for alpha=0.10, found 800 examples\n",
      "Loaded data for alpha=0.15, found 800 examples\n",
      "Successfully loaded data for 6 alpha values\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = \"/home/jholshuijsen/reasoning-reciting-probing/results/chess/intervention/liref\"\n",
    "\n",
    "# Define the alphas to analyze\n",
    "alphas = [-0.10, -0.05, 0.00, 0.05, 0.10, 0.15]\n",
    "\n",
    "# Function to load data for a specific alpha\n",
    "def load_data_for_alpha(alpha):\n",
    "    # Format alpha with 2 decimal places to preserve trailing zeros\n",
    "    alpha_str = f\"{alpha:.2f}\"\n",
    "    alpha_dir = os.path.join(base_dir, alpha_str)\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(alpha_dir):\n",
    "        print(f\"Directory for alpha={alpha_str} not found: {alpha_dir}\")\n",
    "        return None\n",
    "\n",
    "    # Load the first file found (assuming there's only one combined file)\n",
    "    dataset = load_from_disk(alpha_dir)\n",
    "    \n",
    "    print(f\"Loaded data for alpha={alpha_str}, found {len(dataset)} examples\")\n",
    "    return dataset\n",
    "\n",
    "# Load data for all alphas\n",
    "datasets = {}\n",
    "for alpha in alphas:\n",
    "    data = load_data_for_alpha(alpha)\n",
    "    if data:\n",
    "        datasets[alpha] = data\n",
    "\n",
    "\n",
    "print(f\"Successfully loaded data for {len(datasets)} alpha values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /gpfs/home5/jholshuijsen/reasoning-reciting-probing/code/probes/notebooks/utils\n",
      "Does the file exist? False\n",
      "Checking parent directories:\n",
      "  inputs/chess/data/chess_data.jsonl: False\n",
      "  ../inputs/chess/data/chess_data.jsonl: False\n",
      "  ../../inputs/chess/data/chess_data.jsonl: False\n",
      "  ../../../inputs/chess/data/chess_data.jsonl: False\n",
      "Loaded 800 chess questions from ../../../../inputs/chess/data/chess_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# load the jsonl file with the original prompt questions:\n",
    "import json\n",
    "# Print the path to help debug file not found errors\n",
    "import os\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Does the file exist? {os.path.exists('inputs/chess/data/chess_data.jsonl')}\")\n",
    "print(f\"Checking parent directories:\")\n",
    "for i in range(4):  # Check up to 3 levels up\n",
    "    path = os.path.join(*(['..'] * i), 'inputs/chess/data/chess_data.jsonl')\n",
    "    print(f\"  {path}: {os.path.exists(path)}\")\n",
    "\n",
    "\n",
    "# Load the original prompt questions from the jsonl file\n",
    "chess_data_path = \"../../../../inputs/chess/data/chess_data.jsonl\"\n",
    "chess_questions = []\n",
    "\n",
    "with open(chess_data_path, 'r') as f:\n",
    "    for line in f:\n",
    "        chess_questions.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(chess_questions)} chess questions from {chess_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_response(dataset):    \n",
    "    yes_count = 0\n",
    "    no_count = 0\n",
    "    invalid_count = 0\n",
    "    invalid_answers = []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        response = dataset[i]['intervention_response']\n",
    "        \n",
    "        # Extract the assistant's response part\n",
    "        if \"assistant\" in response:\n",
    "            answer_part = response.split(\"assistant\", 1)[1].strip()\n",
    "        else:\n",
    "            print(f\"Warning: Could not identify assistant part in response for row {dataset[i]['row']}\")\n",
    "            invalid_count += 1\n",
    "            invalid_answers.append(response)\n",
    "            results.append(\"invalid\")\n",
    "            continue\n",
    "        \n",
    "        # Look for boxed yes/no\n",
    "        has_yes = r\"\\boxed{yes}\" in answer_part\n",
    "        has_no = r\"\\boxed{no}\" in answer_part\n",
    "        \n",
    "        # Check if both yes and no are present\n",
    "        answer_part = answer_part.replace(\".\", \" \")\n",
    "        answer_part = answer_part.strip()\n",
    "        last_line = answer_part.split(\"\\n\")[-1]\n",
    "        success = False\n",
    "        \n",
    "        if has_yes and has_no:\n",
    "            #print(f\"Warning: Both \\\\boxed{{yes}} and \\\\boxed{{no}} found in answer for row {dataset[i]['row']}\")\n",
    "            invalid_answers.append(answer_part)\n",
    "            invalid_count += 1\n",
    "            results.append('invalid')\n",
    "            continue\n",
    "            \n",
    "        if has_yes:\n",
    "            results.append('yes')\n",
    "            success = True\n",
    "        elif answer_part.endswith('is legal'):\n",
    "            results.append(\"yes\")\n",
    "            success = True\n",
    "        elif 'are legal' in last_line or 'is legal' in last_line:\n",
    "            success = True\n",
    "            results.append(\"yes\")\n",
    "\n",
    "        if has_no:\n",
    "            results.append(\"no\")\n",
    "            success = True\n",
    "        elif answer_part.endswith('illegal') or answer_part.endswith('not legal'):\n",
    "            results.append(\"no\")\n",
    "            success = True\n",
    "        elif 'not legal' in last_line or 'illegal' in last_line or 'not valid' in last_line or 'not a valid' in last_line:\n",
    "            results.append(\"no\")\n",
    "            success = True\n",
    "\n",
    "        if not success:\n",
    "            #print(f\"Warning: Could not identify an answer for row {i}\")\n",
    "            invalid_answers.append(answer_part)\n",
    "            results.append(\"invalid\")\n",
    "\n",
    "    yes_count = results.count(\"yes\")\n",
    "    no_count = results.count(\"no\")\n",
    "    invalid_count = results.count(\"invalid\")\n",
    "    evaluated_answer_count = len(results)\n",
    "    if evaluated_answer_count != len(dataset):\n",
    "        print(\"Warning: some answers seem to be ambiguous as total evaluated answer length\" \n",
    "              f\"{evaluated_answer_count} does not match the dataset length {len(dataset)}\"\n",
    "              )\n",
    "    \n",
    "    print(f\"Yes answers: {yes_count}, No answers: {no_count}, Invalid answers: {invalid_count}\")\n",
    "    \n",
    "    return results, invalid_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy_metrics(dataset, results):\n",
    "    rw_correct_yes = 0\n",
    "    rw_correct_no = 0\n",
    "    rw_incorrect_yes = 0\n",
    "    rw_incorrect_no = 0\n",
    "    cf_correct_yes = 0\n",
    "    cf_correct_no = 0\n",
    "    cf_incorrect_yes = 0\n",
    "    cf_incorrect_no = 0\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if results[i] == 'invalid':\n",
    "            continue\n",
    "        if dataset[i]['mode'] == 'real_world':\n",
    "            if results[i] == 'yes':\n",
    "                if dataset[i]['real_world_answer']:\n",
    "                    rw_correct_yes += 1\n",
    "                else:\n",
    "                    rw_incorrect_yes += 1\n",
    "            elif results[i] == 'no':\n",
    "                if not dataset[i]['real_world_answer']:\n",
    "                    rw_correct_no += 1\n",
    "                else:\n",
    "                    rw_incorrect_no += 1\n",
    "          \n",
    "        if dataset[i]['mode'] == 'counter_factual':\n",
    "            if results[i] == 'yes':\n",
    "                if not dataset[i]['counter_factual_answer']:\n",
    "                    cf_correct_yes += 1\n",
    "                else:\n",
    "                    cf_incorrect_yes += 1\n",
    "            elif results[i] == 'no':\n",
    "                if dataset[i]['counter_factual_answer']:\n",
    "                    cf_correct_no += 1\n",
    "                else:\n",
    "                    cf_incorrect_no += 1\n",
    "     \n",
    "    rw_correct = rw_correct_yes + rw_correct_no\n",
    "    cf_correct = cf_correct_yes + cf_correct_no\n",
    "    rw_incorrect = rw_incorrect_yes + rw_incorrect_no   \n",
    "    cf_incorrect = cf_incorrect_yes + cf_incorrect_no\n",
    "    \n",
    "    print(f\"Real world correct: {rw_correct_yes} yes, {rw_correct_no} no\")\n",
    "    print(f\"Real world incorrect: {rw_incorrect_yes} yes, {rw_incorrect_no} no\")\n",
    "    print(f\"Counter factual correct: {cf_correct_yes} yes, {cf_correct_no} no\")\n",
    "    print(f\"Counter factual incorrect: {cf_incorrect_yes} yes, {cf_incorrect_no} no\")\n",
    "    print(f\"Accuracy real world: {rw_correct / (rw_correct + rw_incorrect) if (rw_correct + rw_incorrect) > 0 else 'N/A'}\")\n",
    "    print(f\"Accuracy counter factual: {cf_correct / (cf_correct + cf_incorrect) if (cf_correct + cf_incorrect) > 0 else 'N/A'}\")\n",
    "    \n",
    "    return {\n",
    "        'rw_correct_yes': rw_correct_yes,\n",
    "        'rw_correct_no': rw_correct_no,\n",
    "        'rw_incorrect_yes': rw_incorrect_yes,\n",
    "        'rw_incorrect_no': rw_incorrect_no,\n",
    "        'cf_correct_yes': cf_correct_yes,\n",
    "        'cf_correct_no': cf_correct_no,\n",
    "        'cf_incorrect_yes': cf_incorrect_yes,\n",
    "        'cf_incorrect_no': cf_incorrect_no,\n",
    "        'rw_accuracy': rw_correct / (rw_correct + rw_incorrect) if (rw_correct + rw_incorrect) > 0 else None,\n",
    "        'cf_accuracy': cf_correct / (cf_correct + cf_incorrect) if (cf_correct + cf_incorrect) > 0 else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating dataset for alpha=-0.1\n",
      "Warning: some answers seem to be ambiguous as total evaluated answer length808 does not match the dataset length 800\n",
      "Yes answers: 315, No answers: 469, Invalid answers: 24\n",
      "Real world correct: 95 yes, 134 no\n",
      "Real world incorrect: 66 yes, 104 no\n",
      "Counter factual correct: 74 yes, 108 no\n",
      "Counter factual incorrect: 80 yes, 115 no\n",
      "Accuracy real world: 0.5739348370927319\n",
      "Accuracy counter factual: 0.4827586206896552\n",
      "Evaluating dataset for alpha=-0.05\n",
      "Warning: some answers seem to be ambiguous as total evaluated answer length802 does not match the dataset length 800\n",
      "Yes answers: 343, No answers: 434, Invalid answers: 25\n",
      "Real world correct: 96 yes, 130 no\n",
      "Real world incorrect: 70 yes, 104 no\n",
      "Counter factual correct: 85 yes, 97 no\n",
      "Counter factual incorrect: 91 yes, 102 no\n",
      "Accuracy real world: 0.565\n",
      "Accuracy counter factual: 0.48533333333333334\n",
      "Evaluating dataset for alpha=0.0\n",
      "Warning: some answers seem to be ambiguous as total evaluated answer length802 does not match the dataset length 800\n",
      "Yes answers: 303, No answers: 485, Invalid answers: 14\n",
      "Real world correct: 95 yes, 127 no\n",
      "Real world incorrect: 72 yes, 102 no\n",
      "Counter factual correct: 65 yes, 125 no\n",
      "Counter factual incorrect: 71 yes, 129 no\n",
      "Accuracy real world: 0.5606060606060606\n",
      "Accuracy counter factual: 0.48717948717948717\n",
      "Evaluating dataset for alpha=0.05\n",
      "Yes answers: 410, No answers: 368, Invalid answers: 22\n",
      "Real world correct: 104 yes, 89 no\n",
      "Real world incorrect: 104 yes, 91 no\n",
      "Counter factual correct: 99 yes, 92 no\n",
      "Counter factual incorrect: 103 yes, 96 no\n",
      "Accuracy real world: 0.49742268041237114\n",
      "Accuracy counter factual: 0.4897435897435897\n",
      "Evaluating dataset for alpha=0.1\n",
      "Yes answers: 450, No answers: 277, Invalid answers: 73\n",
      "Real world correct: 111 yes, 76 no\n",
      "Real world incorrect: 112 yes, 77 no\n",
      "Counter factual correct: 116 yes, 66 no\n",
      "Counter factual incorrect: 111 yes, 58 no\n",
      "Accuracy real world: 0.4973404255319149\n",
      "Accuracy counter factual: 0.5185185185185185\n",
      "Evaluating dataset for alpha=0.15\n",
      "Warning: some answers seem to be ambiguous as total evaluated answer length801 does not match the dataset length 800\n",
      "Yes answers: 126, No answers: 321, Invalid answers: 354\n",
      "Real world correct: 30 yes, 83 no\n",
      "Real world incorrect: 37 yes, 91 no\n",
      "Counter factual correct: 33 yes, 69 no\n",
      "Counter factual incorrect: 26 yes, 77 no\n",
      "Accuracy real world: 0.46887966804979253\n",
      "Accuracy counter factual: 0.4975609756097561\n"
     ]
    }
   ],
   "source": [
    "for alpha, dataset in datasets.items():\n",
    "    print(f\"Evaluating dataset for alpha={alpha}\")\n",
    "    results, invalid_answers = evaluate_llm_response(dataset)\n",
    "    calculate_accuracy_metrics(chess_questions, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'intervention_response'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datasets[0.1]\n",
    "\n",
    "for i in range(len(data)):\n",
    "    print(data[i]['intervention_response'].split(\"assistant\")[1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "reasoning-reciting-probing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
