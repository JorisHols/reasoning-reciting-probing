{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to convert the activations obtained by running Llama 3 8B on the LiReF paper datasets to direction vectors using difference of means and logistic regression probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Load the activations from .pt file\n",
    "base = \"../../../../inputs/chess/interventions/\"\n",
    "#path = \"/Users/jorisholshuijsen/Documents/Artificial Intelligence/Thesis/reasoning-reciting-probing/inputs/chess/interventions/liref_data_activations.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_difference_of_means_directions(hs_cache_no_cot, model_layers_num, mlp_dim_num, reason_indices, memory_indices):\n",
    "\n",
    "    candidate_directions = torch.zeros((model_layers_num, mlp_dim_num), dtype=torch.float64, device='cuda')\n",
    "\n",
    "    # calculating candidate reasoning features\n",
    "    for layer in range(model_layers_num):\n",
    "            \n",
    "        hs_no_cot = hs_cache_no_cot[layer]\n",
    "\n",
    "        #  we store the mean activations in high-precision to avoid numerical issues\n",
    "        reason_hs_no_cot = hs_no_cot[reason_indices, :].to(torch.float64)\n",
    "        #print('reason_hs_no_cot.shape: ',reason_hs_no_cot.shape) reason有点多，memory有点少，需要进一步把数据集做scale up    \n",
    "        memory_hs_no_cot = hs_no_cot[memory_indices, :].to(torch.float64)\n",
    "\n",
    "        mean_reason_hs_no_cot = reason_hs_no_cot.mean(dim=0)\n",
    "        mean_memory_hs_no_cot = memory_hs_no_cot.mean(dim=0)\n",
    "\n",
    "        mean_diff = mean_reason_hs_no_cot - mean_memory_hs_no_cot  #Reasoning features shape: [bsz, dims] \n",
    "        candidate_directions[layer] = mean_diff\n",
    "\n",
    "    return candidate_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['ceval_liberal', 'gsm8k', 'mgsm', 'popqa'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mmlu-pro_3000samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m loaded_dict = torch.load(os.path.join(base, \u001b[33m'\u001b[39m\u001b[33mliref_data_activations.pt\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(loaded_dict.keys())\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m hs_cache_no_cot = \u001b[43mloaded_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmmlu-pro_3000samples\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m \n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(base, \u001b[33m'\u001b[39m\u001b[33mmmlu-pro-3000samples.json\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      8\u001b[39m         sampled_data = json.load(f)\n",
      "\u001b[31mKeyError\u001b[39m: 'mmlu-pro_3000samples'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "loaded_dict = torch.load(os.path.join(base, 'liref_data_activations.pt'))\n",
    "print(loaded_dict.keys())\n",
    "hs_cache_no_cot = loaded_dict['mmlu-pro_3000samples'] \n",
    "\n",
    "with open(os.path.join(base, 'mmlu-pro-3000samples.json'), 'r', encoding='utf-8') as f:\n",
    "        sampled_data = json.load(f)\n",
    "\n",
    "reason_indices = [ix for ix, sample in enumerate(sampled_data) if sample['memory_reason_score'] > 0.5]\n",
    "memory_indices = [ix for ix, sample in enumerate(sampled_data) if sample['memory_reason_score'] <= 0.5]\n",
    "\n",
    "diff_of_means_dirs = get_difference_of_means_directions(hs_cache_no_cot, 32, 4096, reason_indices, memory_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning-reciting-probing--8j9mRgr-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
